#+TITLE: Extending the WebNLG2020 evaluation script to quadruples
#+PROPERTY: header-args:Python :python ./.venv/bin/python3 :exports both
#+LATEX_HEADER: \usepackage{minted}
#+LATEX_HEADER: \usepackage[margin=1.0in]{geometry}

* Inner working of the WebNLG evaluation script

How does the WebNLG script work? Its heart is the ~evaluaterefcand~
function, which is complex. It takes as input two triples, ~reference~
and ~candidate~, each in the following format:

#+begin_src python :session *python* :results none
"subject | predicate | object"
#+end_src

Then, it tries to match elements of these triples together. In the
simplest case, elements correspond to each other:

#+begin_src python :results none
reference = "Linus Torvalds | creator | Linux"
candidate = "Linus Torvalds | creator | Linux"
#+end_src

But they can also be swapped:

#+begin_src python :results none
# note how subject and object are swapped in the candidate triple!
reference = "Linus Torvalds | creator | Linux"
candidate = "Linux          | creator | Linus Torvalds"
#+end_src

In that case, the metric will still attribute partial score depending
on the setting. It is also possible to have a partial match for an
element:

#+begin_src python :results none
# the candidate object "Torvalds" partially matches the reference
# subject
reference = "Linus Torvalds | creator | Linux"
candidate = "Linux          | creator | Torvalds"
#+end_src


** ~evaluaterefcand~ in details

Basically, what does ~evaluaterefcand~ do?

1. Go over the natural pairs of elements (=(SUB, SUBj)=, =(PRED,
   PRED)=, =(OBJ, OBJ)=) and try to find a match between them by
   iterating over n-grams.
2. Convert these match into NER predictions in prodigy spans
   format. For example, for the reference subject ="Linus Torvalds"=
   and the candidate subject ="Linus"= , the reference NER span is
   ={"label": "SUB", "start": 0, "end": 1}=, and the candidate NER
   span is ={"label": "SUB", "start": 0, "end": 0}=. This corresponds
   to =B-SUB I-SUB= and =B-SUB O= in the BIO format respectively.
4. Repeat 1-2 for swapped pairs when an attribute has no match (=(SUB,
   OBJ), (SUB, PRED), (PRED, OBJ)=). If we find a match for these
   pairs, we keep its corresponding NER span.
5. finally, evaluation is carried using [[https://github.com/MantisAI/nervaluate][nervaluate]].

** NERValuate

NERValuate is a NER evaluation library that goes beyond standard
precision, recall and F1, and consider entity boundaries and type more
precisely. It has four evaluation modes: =strict=, =exact=, =partial=
and =type=. For our purpose, this is what it amounts to:

| Mode    | attribute string | attribute type (i.e. position in the triple/quadruple) |
|---------+------------------+--------------------------------------------------------|
| Strict  | exact match      | exact match                                            |
| Exact   | exact match      | ignore                                                 |
| Type    | partial match    | exact match                                            |
| Partial | partial match    | ignore                                                 |

* First idea: building on the existing evaluaterefcand implementation

Our first idea was to build on the existing implementation, extending
it for quadruples. Since the existing ~evaluaterefcand~ function is
complex, a good first step is to write a cleaned up simplified version
of the function by analyzing the logic of the existing one and slowly
reducing it.

** Simplifying ~evaluaterefcand~ 

By slowly simplifying each step of the function while keeping the same
logic, we arrived at this first version:

#+begin_src python :session *python* :results none
from typing import Literal
import nltk, re, string
from nltk.util import ngrams
from evaluate.evaluation_script import getrefdict, nonrefwords

AttrType = Literal["SUB", "PRED", "OBJ"]
yn = Literal["y", "n"]

def cleanup_tokens(tokens: list[str]) -> list[str]:
    return [
        x.lower()
        for x in tokens
        if re.search(r"^[" + re.escape(string.punctuation) + r"]+$", x) == None
    ]

def evaluaterefcand(reference: str, candidate: str) -> tuple[dict, dict]:
    ref = reference.split(" | ")
    cand = candidate.split(" | ")

    attr_types: list[AttrType] = ["SUB", "PRED", "OBJ"]
    attr2index = {k: i for i, k in enumerate(attr_types)}

    # Make sure that reference or candidate aren't '' values originally.
    if len(ref) < 1 or len(cand) < 1:
        if len(ref) == 1:
            ref = ["", "", ""]
        else:
            cand = ["", "", ""]

    refdicts_dict: dict[AttrType, list] = {"SUB": [], "PRED": [], "OBJ": []}
    canddicts_dict: dict[AttrType, list] = {"SUB": [], "PRED": [], "OBJ": []}
    totallist_dict: dict[AttrType, list] = {"SUB": [], "PRED": [], "OBJ": []}
    found: dict[AttrType, yn] = {"SUB": "n", "PRED": "n", "OBJ": "n"}

    # Let's go over each attribute of the triple one by one
    for attr_i, attr_type in enumerate(attr_types):
        refsub = ref[attr_i]
        candsub = cand[attr_i]

        reflist = cleanup_tokens(nltk.word_tokenize(refsub))
        candlist = cleanup_tokens(nltk.word_tokenize(candsub))

        reflist, candlist = nonrefwords(reflist, candlist, 1, len(candlist))

        candidatefound, refdicts, canddicts, totallist = getrefdict(
            reflist,
            candlist,
            attr_type,
            attr_type,
            sum(len(lst) for lst in totallist_dict.values()),
        )
        found[attr_type] = candidatefound
        refdicts_dict[attr_type] = refdicts
        canddicts_dict[attr_type] = canddicts
        totallist_dict[attr_type] = totallist

    # If no matches were found for two or more attributes, we are
    # going to try and compare different attributes to each other.
    swap_pairs = [
        ("SUB", "OBJ"),
        ("SUB", "PRED"),
        ("PRED", "OBJ"),
    ]
    for attr1, attr2 in swap_pairs:
        if (found[attr1] == "y") or (found[attr2] == "y"):
            continue

        refsub = ref[attr2index[attr1]]
        candsub = cand[attr2index[attr2]]
        reflist = cleanup_tokens(nltk.word_tokenize(refsub))
        candlist = cleanup_tokens(nltk.word_tokenize(candsub))

        newreflist, newcandlist = nonrefwords(reflist, candlist, 1, len(candlist))
        offset = sum(
            len(lst)
            for attr, lst in totallist_dict.items()
            if attr2index[attr] < attr2index[attr1] and not attr == attr2
        )
        candidatefound, refdicts, canddicts, totallist = getrefdict(
            newreflist, newcandlist, attr1, attr2, offset
        )

        refsub = ref[attr2index[attr2]]
        candsub = cand[attr2index[attr1]]
        reflist = cleanup_tokens(nltk.word_tokenize(refsub))
        candlist = cleanup_tokens(nltk.word_tokenize(candsub))

        newreflist, newcandlist = nonrefwords(reflist, candlist, 1, len(candlist))
        offset = len(totallist) + sum(
            len(lst)
            for attr, lst in totallist_dict.items()
            if attr2index[attr] < attr2index[attr2] and not attr == attr1
        )
        candidatefound2, refdicts2, canddicts2, totallist2 = getrefdict(
            newreflist, newcandlist, attr2, attr1, offset
        )

        if (candidatefound == "y") or (candidatefound2 == "y"):
            found[attr1] = candidatefound
            refdicts_dict[attr1] = refdicts
            canddicts_dict[attr1] = canddicts
            totallist_dict[attr1] = totallist

            found[attr2] = candidatefound2
            refdicts_dict[attr2] = refdicts2
            canddicts_dict[attr2] = canddicts2
            totallist_dict[attr2] = totallist2

            # update entities that were "sandwiched" between attr1 and attr2
            attrs_between: list[AttrType] = [
                a
                for a in attr_types
                if attr2index[a] < attr2index[attr2]
                and attr2index[a] > attr2index[attr1]
            ]
            for attr in set(attrs_between):
                offset = sum(
                    len(lst)
                    for other_attr, lst in totallist_dict.items()
                    if attr2index[other_attr] < attr2index[attr]
                )
                candidatefound, refdicts, canddicts, totallist = getrefdict(
                    newreflist, newcandlist, attr, attr, offset
                )
                found[attr] = candidatefound
                refdicts_dict[attr] = refdicts
                canddicts_dict[attr] = canddicts
                totallist_dict[attr] = totallist

            break

    allrefdict = list(ft.reduce(add, [refdicts_dict[attr] for attr in attr_types]))
    allcanddict = list(ft.reduce(add, [canddicts_dict[attr] for attr in attr_types]))

    # Returns overall metrics and metrics for each tag
    evaluator = Evaluator([allrefdict], [allcanddict], tags=attr_types)
    results, results_per_tag = evaluator.evaluate()

    return results, results_per_tag
#+end_src

This is still pretty complex!

This implementation is empirically proven to be equivalent to the old
implementation thanks to =hypothesis=:

#+begin_src python :session *python* :results pp
from hypothesis import given, strategies as st
from string import ascii_letters
from evaluate.archive import evaluaterefcand as evaluaterefcand_old
from evaluate.evaluation_script import evaluaterefcand


@st.composite
def st_triples(draw, **kwargs) -> str:
    sub = draw(st.text(alphabet=ascii_letters, **kwargs))
    pred = draw(st.text(alphabet=ascii_letters, **kwargs))
    obj = draw(st.text(alphabet=ascii_letters, **kwargs))
    return f"{sub} | {pred} | {obj}"


@given(st_triples(min_size=1), st_triples(min_size=1))
def test_retrocompatible(ref: str, cand: str):
    old_out = evaluaterefcand_old(ref, cand)
    new_out = evaluaterefcand(ref, cand)
    assert old_out == new_out

test_retrocompatible()
#+end_src

#+RESULTS:
: None

Arrived there, we strived to simplify the function further. We cut it
into several pieces. First, we noticed that in the legacy function,
computing the match between two triple elements is repeated and looks
like this:

#+begin_src python :session *python* :results pp
def cand_ner_spans(
    ref: str,
    cand: str,
    attr_type: AttrType,
    totallist_dict: dict[AttrType, list],
) -> tuple[yn, dict, dict, dict]:

    reflist = cleanup_tokens(nltk.word_tokenize(ref))
    candlist = cleanup_tokens(nltk.word_tokenize(cand))

    reflist, candlist = nonrefwords(reflist, candlist, 1, len(candlist))

    candfound, refdicts, canddicts, totallist = getrefdict(
        reflist,
        candlist,
        attr_type,
        attr_type,
        sum(len(lst) for lst in totallist_dict.values()),
    )

    return candfound, refdicts, canddicts, totallist

cand_ner_spans("Linus", "Linus Torvalds", "SUB", {})
#+end_src

#+RESULTS:
: ('y',
:  [{'end': 0, 'label': 'SUB', 'start': 0}],
:  [{'end': 1, 'label': 'SUB', 'start': 0}],
:  ['FOUNDREF-1-0', 'FOUNDCAND-1-LINKED'])

Admittedly, this simplified version is already a bit complex. A source
of complexity in the script is dealing with offsets (the last argument
of ~getrefdict~). We could deal with offsets at the end of the
function, when all alignments are done. With some additional
simplifications and added clarity of a return type, that would give
us:

#+begin_src python :session *python* :results pp
from typing import TypedDict
from dataclasses import dataclass

class NERSpan(TypedDict):
    label: AttrType
    start: int
    end: int

@dataclass
class NERSpansMatch:
    found: yn
    ref_dicts: list[NERSpan] # in practice, the length is always 1?
    cand_dicts: list[NERSpan] # same

def attr_ner_spans(ref: str, cand: str, attr_type: AttrType) -> NERSpansMatch:
    reflist = cleanup_tokens(nltk.word_tokenize(ref))
    candlist = cleanup_tokens(nltk.word_tokenize(cand))

    reflist, candlist = nonrefwords(reflist, candlist, 1, len(candlist))
    candfound, refdicts, canddicts, _ = getrefdict(
        reflist, candlist, attr_type, attr_type, 0
    )

    return NERSpansMatch(candfound, refdicts, canddicts)

vars(attr_ner_spans("Linus", "Linus Torvalds", "SUB"))
#+end_src

#+RESULTS:
: {'cand_dicts': [{'end': 1, 'label': 'SUB', 'start': 0}],
:  'found': 'y',
:  'ref_dicts': [{'end': 0, 'label': 'SUB', 'start': 0}]}

To deal with possible swaps, the original code is so complex it's hard
to extract in a function. Without offset, it's simpler:

#+begin_src python :session *python* :results pp
def _swapped_ner_spans(
    ref: str, cand: str, attr_type1: AttrType, attr_type2: AttrType
) -> NERSpansMatch:
    reflist = cleanup_tokens(nltk.word_tokenize(ref))
    candlist = cleanup_tokens(nltk.word_tokenize(cand))

    reflist, candlist = nonrefwords(reflist, candlist, 1, len(candlist))
    candfound, refdicts, canddicts, _ = getrefdict(
        reflist, candlist, attr_type1, attr_type2, 0
    )

    return NERSpansMatch(candfound, refdicts, canddicts)


def swapped_ner_spans(
    ref1: str,
    cand1: str,
    ref2: str,
    cand2: str,
    attr_type1: AttrType,
    attr_type2: AttrType,
) -> tuple[NERSpansMatch, NERSpansMatch]:
    return (
      _swapped_ner_spans(ref1, cand1, attr_type1, attr_type2),
      _swapped_ner_spans(ref2, cand2, attr_type2, attr_type1),
    ) 

# corresponds to this example:
# Reference = "Linus Torvalds | creator | Linux"
# candidate = "Linux          | creator | Torvalds"
ref_spans, cand_spans = swapped_ner_spans(
    "Linus Torvalds", "Linux", "Linux", "Torvalds", "SUB", "OBJ"
)
(vars(ref_spans), vars(cand_spans))
#+end_src

#+RESULTS:
: ({'cand_dicts': [{'end': 2, 'label': 'OBJ', 'start': 2}],
:   'found': 'n',
:   'ref_dicts': [{'end': 1, 'label': 'SUB', 'start': 0}]},
:  {'cand_dicts': [{'end': 1, 'label': 'SUB', 'start': 1}],
:   'found': 'n',
:   'ref_dicts': [{'end': 0, 'label': 'OBJ', 'start': 0}]})

Finally, we arrive at this version:

#+begin_src python :session *python* :results none
import functools as ft
from operator import add
from nervaluate import Evaluator


def parse_triple(triple: str) -> dict[AttrType, str]:
    split = triple.split(" | ")
    if len(split) < 1:
        return {"SUB": "", "PRED": "", "OBJ": ""}
    return {"SUB": split[0], "PRED": split[1], "OBJ": split[2]}


def change_dicts_label(dicts: list[dict], new_label: str) -> dict:
    return [{"label": new_label, "start": d["start"], "end": d["end"]} for d in dicts]


def cand_ner_spans(
    ref: dict[AttrType, str], cand: dict[AttrType, str]
) -> tuple[dict, dict]:
    attr_types = ["SUB", "PRED", "OBJ"]
    ref_dict = {}
    cand_dict = {}
    match_dict = {}
    found_dict = {}

    for attr_type in attr_types:
        match_ = attr_ner_spans(ref[attr_type], cand[attr_type], attr_type)
        found_dict[attr_type] = match_.found
        ref_dict[attr_type] = match_.ref_dicts
        cand_dict[attr_type] = match_.cand_dicts

    swap_pairs = [
        ("SUB", "OBJ"),
        ("SUB", "PRED"),
        ("PRED", "OBJ"),
    ]
    for attr_type1, attr_type2 in swap_pairs:
        if (
            found_dict[match_dict.get(attr_type1, attr_type1)] == "y"
            or found_dict[match_dict.get(attr_type2, attr_type2)] == "y"
        ):
            continue

        # check found
        match1, match2 = swapped_ner_spans(
            ref[attr_type1],
            cand[attr_type2],
            ref[attr_type2],
            cand[attr_type1],
            attr_type1,
            attr_type2,
        )
        if match1.found == "y" or match2.found == "y":
            # update1
            found_dict[attr_type1] = match1.found
            ref_dict[attr_type1] = match1.ref_dicts
            cand_dict[attr_type1] = change_dicts_label(
                match1.cand_dicts, match_dict.get(attr_type2, attr_type2)
            )
            # update2
            found_dict[attr_type2] = match2.found
            ref_dict[attr_type2] = match2.ref_dicts
            cand_dict[attr_type2] = change_dicts_label(
                match2.cand_dicts, match_dict.get(attr_type1, attr_type1)
            )

            match_dict[attr_type1] = attr_type2
            match_dict[attr_type2] = attr_type1

    # update indices
    offset = 0
    for attr_type in attr_types:
        for d in ref_dict[attr_type]:
            d["start"] += offset
            d["end"] += offset
        for d in cand_dict[attr_type]:
            d["start"] += offset
            d["end"] += offset
        ref_offset = max(d["end"] for d in ref_dict[attr_type]) + 1
        cand_offset = max(d["end"] for d in cand_dict[attr_type]) + 1
        offset = max(ref_offset, cand_offset)

    # end
    return (
        list(ft.reduce(add, [ref_dict[attr] for attr in attr_types])),
        list(ft.reduce(add, [cand_dict[attr] for attr in attr_types])),
    )


def evaluaterefcand(ref: str, cand: str) -> tuple[dict, dict]:
    """
    :return: (results, results_per_tag)
    """
    ref_dict, cand_dict = cand_ner_spans(parse_triple(ref), parse_triple(cand))

    evaluator = Evaluator([ref_dict], [cand_dict], tags=["SUB", "PRED", "OBJ"])
    return evaluator.evaluate()
#+end_src

Honestly, this is still too complex and unclear. But at least this
seems to work fine:

#+begin_src python :session *python* :results pp
test_retrocompatible()
#+end_src

#+RESULTS:
: None

However, we were quickly stopped in our track by an unforseen issue...

** The WebNLG evaluation function is wrong

While fiddling with the function, we discovered an issue: the WebNLG
function is actually behaving /incorrectly/. To see that, it suffices
to show this example:

#+begin_src python :session *python* :results pp
ref = "A | B | C"
cand = "C | A | B"
evaluaterefcand_old(ref, cand)[0]["ent_type"]["correct"]
#+end_src

#+RESULTS:
: 1

In the candidate triple, /no elements are aligned!/ so the =type=
score should be exactly 0. We can kind of see what happens if we check
what happens in the function's internals. We split the legacy function
to have ~evaluaterefcand_core_old~ return the NER spans sent to
NERValuate:

#+begin_src python :session *python* :results pp
from evaluate.archive import evaluaterefcand_core as evaluaterefcand_core_old

evaluaterefcand_core_old(ref, cand)
#+end_src

#+RESULTS:
: ([{'end': 0, 'label': 'SUB', 'start': 0},
:   {'end': 2, 'label': 'PRED', 'start': 2},
:   {'end': 4, 'label': 'OBJ', 'start': 4}],
:  [{'end': 1, 'label': 'OBJ', 'start': 1},
:   {'end': 2, 'label': 'PRED', 'start': 2},
:   {'end': 4, 'label': 'SUB', 'start': 4}])


Somehow, the function aligns the reference and the candidate
predicate, even though they are completely different.

There is another issue we found with the WebNLG implementation, which
is more on the design side: Why check possible swaps in a specific
order =SUB-OBJ, SUB-PRED, PRED-OBJ=, and stop at the first swap that
yields a partial match? This means the test order of the swaps has an
impact on the final score, which in our opinion is an incorrect
behaviour.

* A New Implementation

We propose the following implementation. The major difference is in
the design: instead of relying on predefined swaps between attribute
types, we instead keep the best possible alignment for the =exact= and
=partial= metrics. This allows us to easily extend the function to
support timestamps.

#+begin_src python :session *python* :results pp
import itertools as it
from evaluate.archive import evaluaterefcand as evaluaterefcand_old
from nervaluate import Evaluator


def parse_triple(triple: str) -> list[str]:
    split = triple.split(" | ")
    if len(split) < 1:
        return ["", "", ""]
    return split


def _swapped_ner_spans(
    ref: str, cand: str, attr_type1: AttrType, attr_type2: AttrType, offset: int
) -> NERSpansMatch:
    reflist = cleanup_tokens(nltk.word_tokenize(ref))
    candlist = cleanup_tokens(nltk.word_tokenize(cand))

    reflist, candlist = nonrefwords(reflist, candlist, 1, len(candlist))
    candfound, refdicts, canddicts, _ = getrefdict(
        reflist, candlist, attr_type1, attr_type2, offset
    )

    return NERSpansMatch(candfound, refdicts, canddicts)


def evaluaterefcand(reference: str, candidate: str) -> tuple[dict, dict]:
    ref = parse_triple(reference)
    cand = parse_triple(candidate)
    assert len(ref) == len(cand)
    if len(ref) == 3:
        attr_types = ["SUB", "PRED", "OBJ"]
    elif len(ref) == 4:
        attr_types = ["SUB", "PRED", "OBJ", "TS"]
    else:
        raise ValueError(f"invalid n-tuple length: {len(ref)}")


    best_scores = (
        {"exact": {"f1": 0.0}, "partial": {"f1": 0.0}},
        {attr_type: {} for attr_type in attr_types},
    )

    for cand_permut, cand_attr_types_permut in zip(
        it.permutations(cand), it.permutations(attr_types)
    ):
        offset = 0
        ref_dict = {}
        cand_dict = {}
        for ref_attr, cand_attr, ref_attr_type, cand_attr_type in zip(
            ref, cand_permut, attr_types, cand_attr_types_permut
        ):
            match_ = _swapped_ner_spans(
                ref_attr, cand_attr, ref_attr_type, cand_attr_type, offset
            )
            ref_dict[ref_attr_type] = match_.ref_dicts
            cand_dict[cand_attr_type] = match_.cand_dicts
            offset = (
                max(
                    max(d["end"] for d in match_.ref_dicts),
                    max(d["end"] for d in match_.cand_dicts),
                )
                + 1
            )

        ref_list = list(ft.reduce(add, [ref_dict[attr] for attr in attr_types]))
        cand_list = list(ft.reduce(add, [cand_dict[attr] for attr in attr_types]))
        scores = Evaluator([ref_list], [cand_list], tags=attr_types).evaluate()

        # This is the default alignment: we use it to obtain "strict"
        # and "ent_type" scores since these depends on the candidate
        # alignment.
        if cand_attr_types_permut == tuple(attr_types):
            best_scores[0]["strict"] = scores[0]["strict"]
            best_scores[0]["ent_type"] = scores[0]["ent_type"]
            for attr_type in attr_types:
                best_scores[1][attr_type]["strict"] = scores[1][attr_type]["strict"]
                best_scores[1][attr_type]["ent_type"] = scores[1][attr_type]["ent_type"]

        # For "exact" and "partial" scores, we are allowed to search
        # for the best alignment. If this alignment is the best so
        # far, we update these scores. We prioritize "exact" F1 and
        # break ties with "partial" F1.
        if scores[0]["exact"]["f1"] > best_scores[0]["exact"]["f1"] or (
            scores[0]["exact"]["f1"] == best_scores[0]["exact"]["f1"]
            and scores[0]["partial"]["f1"] > best_scores[0]["partial"]["f1"]
        ):
            best_scores[0]["exact"] = scores[0]["exact"]
            best_scores[0]["partial"] = scores[0]["partial"]
            for attr_type in attr_types:
                best_scores[1][attr_type]["exact"] = scores[1][attr_type]["exact"]
                best_scores[1][attr_type]["partial"] = scores[1][attr_type]["partial"]

    return best_scores


ref = "A | B | C"
cand = "A | B | C"
evaluaterefcand(ref, cand)[0]
#+end_src

#+RESULTS:
#+begin_example
{'ent_type': {'actual': 3,
              'correct': 3,
              'f1': 1.0,
              'incorrect': 0,
              'missed': 0,
              'partial': 0,
              'possible': 3,
              'precision': 1.0,
              'recall': 1.0,
              'spurious': 0},
 'exact': {'actual': 3,
           'correct': 3,
           'f1': 1.0,
           'incorrect': 0,
           'missed': 0,
           'partial': 0,
           'possible': 3,
           'precision': 1.0,
           'recall': 1.0,
           'spurious': 0},
 'partial': {'actual': 3,
             'correct': 3,
             'f1': 1.0,
             'incorrect': 0,
             'missed': 0,
             'partial': 0,
             'possible': 3,
             'precision': 1.0,
             'recall': 1.0,
             'spurious': 0},
 'strict': {'actual': 3,
            'correct': 3,
            'f1': 1.0,
            'incorrect': 0,
            'missed': 0,
            'partial': 0,
            'possible': 3,
            'precision': 1.0,
            'recall': 1.0,
            'spurious': 0}}
#+end_example


Let's validate some properties of our metric using tests:

#+begin_src python :session *python* :results pp
from hypothesis import assume
from datetime import datetime, timedelta
from random import randrange

@st.composite
def st_timestamps(draw, start: datetime, end: datetime) -> str:
    delta = end - start
    out_date = start + timedelta(
        days=draw(st.integers(min_value=0, max_value=delta.days))
    )
    return out_date.strftime("%Y-%m-%d")

@st.composite
def st_quads(draw, **kwargs) -> str:
    sub = draw(st.text(alphabet=ascii_letters, **kwargs))
    pred = draw(st.text(alphabet=ascii_letters, **kwargs))
    obj = draw(st.text(alphabet=ascii_letters, **kwargs))
    ts = draw(st_timestamps(datetime(1900, 1, 1), datetime(2050, 1, 1)))
    return f"{sub} | {pred} | {obj} | {ts}"

@given(st_quads(min_size=1))
def test_perfect_cand_gives_perfect_score(ref: str):
    results, _ = evaluaterefcand(ref, ref)
    for metric_mode in ["strict", "exact", "partial", "ent_type"]:
        assert results[metric_mode]["precision"] == 1.0
        assert results[metric_mode]["recall"] == 1.0
        assert results[metric_mode]["f1"] == 1.0

def swap_attrs(quad: str, i: int, j: int) -> str:
    swapped = quad.split(" | ")
    tmp = swapped[i]
    swapped[i] = swapped[j]
    swapped[j] = tmp
    swapped = " | ".join(swapped)
    return swapped

@given(
    st_quads(min_size=1),
    st.integers(min_value=0, max_value=3),
    st.integers(min_value=0, max_value=3),
)
def test_swap_perfect_cand_attrs_gives_perfect_exact_and_partial_score(ref: str, i: int, j: int):
    assume(i != j)
    cand = swap_attrs(ref, i, j)
    results, _ = evaluaterefcand(ref, cand)
    assert results["exact"]["f1"] == 1.0
    assert results["partial"]["f1"] == 1.0

@given(st_quads(min_size=1))
def test_unaligned_elements_gives_null_strict_and_type_score(ref: str):
    assume(len(set(ref.split(" | "))) == 4)
    cand = swap_attrs(ref, 0, 2)
    cand = swap_attrs(cand, 1, 3)
    results, _ = evaluaterefcand(ref, cand)
    assert results["ent_type"]["f1"] == 0.0
    assert results["strict"]["f1"] == 0.0

@given(st_quads(min_size=1), st_quads(min_size=1))
def test_score_ordering_is_correct(ref: str, cand: str):
    results, _ = evaluaterefcand(ref, cand)
    assert results["ent_type"]["f1"] >= results["strict"]["f1"]
    assert results["exact"]["f1"] >= results["strict"]["f1"]

test_perfect_cand_gives_perfect_score()   
test_swap_perfect_cand_attrs_gives_perfect_exact_and_partial_score()
test_unaligned_elements_gives_null_strict_and_type_score()
#+end_src

#+RESULTS:
: None
